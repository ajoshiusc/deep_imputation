{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtiAWdGKX7J"
      },
      "source": [
        "# Deep Imputation of BraTS dataset with MONAI\n",
        "\n",
        "The dataset comes from http://medicaldecathlon.com/.  \n",
        "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
        "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
        "Source: BRATS 2016 and 2017 datasets.  \n",
        "Challenge: **Drop some of the modalities randomly and reconstruct it by imputing with a 3D U-Net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from logger import Logger\n",
        "logger = Logger(log_level='DEBUG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set parameters\n",
        "\n",
        "* RUN_ID : set this to prevent overlapped saving of model and data\n",
        "\n",
        "* DO_MASK : Set to True if mask is to be applied while training\n",
        "* SET_VARIANCE : Set to True if variance is to be trained in loss function\n",
        "* PIXEL_DOWNSAMPLE : How much to scale down each axis. In other words, mm per pixel/voxel\n",
        "\n",
        "* MAX_EPOCHS\n",
        "* VAL_INTERVAL : how frequently the validation code should be run\n",
        "* TRAIN_RATIO: proportion of total dataset to be used for training. Rest will be used for validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_ID = 82\n",
        "QR_REGRESSION = True\n",
        "DO_MASK = True\n",
        "SET_VARIANCE = True\n",
        "MAX_EPOCHS = 6000\n",
        "TRAIN_DATA_SIZE = 50\n",
        "BATCHSIZE_TRAIN = 2\n",
        "# PIXEL_DOWNSAMPLE = [2, 2, 2]\n",
        "VAL_INTERVAL = 10\n",
        "TRAIN_RATIO = 0.8\n",
        "RANDOM_SEED = 0\n",
        "CONTINUE_TRAINING = False\n",
        "TRAIN_DATA_SHUFFLE = True\n",
        "ROOT_DIR = \"/scratch1/sachinsa/monai_data_1\"\n",
        "\n",
        "# test code sanity (for silly errors)\n",
        "SANITY_CHECK = False\n",
        "if SANITY_CHECK:\n",
        "    RUN_ID = 0\n",
        "    MAX_EPOCHS = 15\n",
        "    TRAIN_DATA_SIZE = 10\n",
        "    VAL_INTERVAL = 2\n",
        "\n",
        "params = {\n",
        "    'RUN_ID': RUN_ID,\n",
        "    'QR_REGRESSION': QR_REGRESSION,\n",
        "    'DO_MASK': DO_MASK,\n",
        "    'SET_VARIANCE': SET_VARIANCE,\n",
        "    'MAX_EPOCHS': MAX_EPOCHS,\n",
        "    'ROOT_DIR': ROOT_DIR\n",
        "}\n",
        "\n",
        "logger.info(\"PARAMETERS\\n-----------------\")\n",
        "logger.info(f\"RUN_ID: {RUN_ID}\")\n",
        "logger.info(f\"QR_REGRESSION: {QR_REGRESSION}\")\n",
        "logger.info(f\"DO_MASK: {DO_MASK}\")\n",
        "logger.info(f\"SET_VARIANCE: {SET_VARIANCE}\")\n",
        "logger.info(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
        "logger.info(f\"TRAIN_DATA_SIZE: {TRAIN_DATA_SIZE}\")\n",
        "logger.info(f\"BATCHSIZE_TRAIN: {BATCHSIZE_TRAIN}\")\n",
        "# logger.info(f\"PIXEL_DOWNSAMPLE: {PIXEL_DOWNSAMPLE}\")\n",
        "logger.info(f\"VAL_INTERVAL: {VAL_INTERVAL}\")\n",
        "logger.info(f\"TRAIN_RATIO: {TRAIN_RATIO}\")\n",
        "logger.info(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
        "logger.info(f\"CONTINUE_TRAINING: {CONTINUE_TRAINING}\")\n",
        "logger.info(f\"TRAIN_DATA_SHUFFLE: {TRAIN_DATA_SHUFFLE}\")\n",
        "logger.info(f\"ROOT_DIR: {ROOT_DIR}\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check if this is a notebook or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_notebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True  # Jupyter notebook or Jupyter QtConsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other types\n",
        "    except NameError:\n",
        "        return False  # Standard Python interpreter\n",
        "\n",
        "# Example usage\n",
        "if is_notebook():\n",
        "    logger.debug(\"This is a Jupyter Notebook.\")\n",
        "else:\n",
        "    logger.debug(\"This is a Python script (not a Jupyter Notebook).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDR2QhuhKX7O"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uzoUh4uKX7P",
        "outputId": "8bb638f6-dd68-4ffd-84c2-2d30953b9190",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pdb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.transforms import (\n",
        "    Compose,\n",
        ")\n",
        "from monai.networks.nets import UNet\n",
        "from monai.transforms import (\n",
        "    LoadImage,\n",
        "    Resize,\n",
        "    NormalizeIntensity,\n",
        "    Orientation,\n",
        "    RandFlip,\n",
        "    RandScaleIntensity,\n",
        "    RandShiftIntensity,\n",
        "    RandSpatialCrop,\n",
        "    CenterSpatialCrop,\n",
        "    Spacing,\n",
        "    EnsureType,\n",
        "    EnsureChannelFirst,\n",
        ")\n",
        "from monai.metrics import MSEMetric\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from dataset import BrainMRIDataset\n",
        "\n",
        "# print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = os.path.join(ROOT_DIR, f\"run_{RUN_ID}\")\n",
        "if not CONTINUE_TRAINING and os.path.exists(save_dir) and os.path.isdir(save_dir) and len(os.listdir(save_dir)) != 0:\n",
        "    logger.warning(f\"{save_dir} already exists. Avoid overwrite by updating RUN_ID.\")\n",
        "    # exit()\n",
        "else:\n",
        "    os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXLaSctjKX7Q"
      },
      "source": [
        "### Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeNCIs06KX7R"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Yob6T4xKX7S"
      },
      "source": [
        "## Setup transforms for training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1zn16zLKX7S"
      },
      "outputs": [],
      "source": [
        "# crop_size = [224, 224, 144]\n",
        "# resize_size = [crop_size[i]//PIXEL_DOWNSAMPLE[i] for i in range(len(crop_size))]\n",
        "resize_size = [64,64,64]\n",
        "\n",
        "train_transform = Compose(\n",
        "    [\n",
        "        # load 4 Nifti images and stack them together\n",
        "        LoadImage(),\n",
        "        EnsureChannelFirst(),\n",
        "        EnsureType(),\n",
        "        Orientation(axcodes=\"RAS\"),\n",
        "        # Spacing(\n",
        "        #     pixdim=(1.0, 1.0, 1.0),\n",
        "        #     mode=(\"bilinear\", \"nearest\"),\n",
        "        # ),\n",
        "        # RandSpatialCrop(roi_size=crop_size, random_size=False),\n",
        "        Resize(spatial_size=resize_size, mode='nearest'),\n",
        "        # RandFlip(prob=0.5, spatial_axis=0),\n",
        "        # RandFlip(prob=0.5, spatial_axis=1),\n",
        "        # RandFlip(prob=0.5, spatial_axis=2),\n",
        "        NormalizeIntensity(nonzero=True, channel_wise=True),\n",
        "        # RandScaleIntensity(factors=0.1, prob=1.0),\n",
        "        # RandShiftIntensity(offsets=0.1, prob=1.0),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = Compose(\n",
        "    [\n",
        "        LoadImage(),\n",
        "        EnsureChannelFirst(),\n",
        "        EnsureType(),\n",
        "        Orientation(axcodes=\"RAS\"),\n",
        "        # Spacing(\n",
        "        #     pixdim=(1.0, 1.0, 1.0),\n",
        "        #     mode=(\"bilinear\", \"nearest\"),\n",
        "        # ),\n",
        "        # CenterSpatialCrop(roi_size=crop_size), # added this because model was not handling 155dims\n",
        "        Resize(spatial_size=resize_size, mode='nearest'),\n",
        "        NormalizeIntensity(nonzero=True, channel_wise=True),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dhVJ9qbVbGR"
      },
      "source": [
        "Create training and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRkpcvfEHY7Q"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "all_dataset = BrainMRIDataset(\n",
        "    root_dir=ROOT_DIR,\n",
        "    seed = RANDOM_SEED\n",
        ")\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = random_split(all_dataset, [TRAIN_RATIO, 1-TRAIN_RATIO],\n",
        "                                          generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
        "\n",
        "train_dataset.dataset.transform = train_transform\n",
        "val_dataset.dataset.transform = val_transform\n",
        "\n",
        "if TRAIN_DATA_SIZE:\n",
        "    train_dataset = Subset(train_dataset, list(range(TRAIN_DATA_SIZE)))\n",
        "    val_dataset = Subset(val_dataset, list(range(TRAIN_DATA_SIZE//4)))\n",
        "\n",
        "BATCHSIZE_VAL = BATCHSIZE_TRAIN\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE_TRAIN, shuffle=TRAIN_DATA_SHUFFLE,\n",
        "    num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE_VAL, shuffle=False, num_workers=8)\n",
        "\n",
        "logger.debug(\"Data loaded\")\n",
        "logger.debug(f\"Length of dataset: {len(train_dataset)}, {len(val_dataset)}\")\n",
        "logger.debug(f\"Batch-size: {BATCHSIZE_TRAIN}, {BATCHSIZE_VAL}\")\n",
        "logger.debug(f\"Length of data-loaders: {len(train_loader)}, {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXd4zV8DKX7S"
      },
      "source": [
        "## Check data shape and visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnvWc-L1KX7S"
      },
      "outputs": [],
      "source": [
        "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
        "if is_notebook():\n",
        "    channels = [\"FLAIR\", \"T1w\", \"T1gd\", \"T2w\"]\n",
        "    val_data_example = val_dataset[0]['image']\n",
        "    _, im_length, im_width, im_height = val_data_example.shape\n",
        "    logger.debug(f\"image shape: {val_data_example.shape}\")\n",
        "    plt.figure(\"image\", (24, 6))\n",
        "    for i in range(4):\n",
        "        plt.subplot(1, 4, i + 1)\n",
        "        plt.title(channels[i], fontsize=30)\n",
        "        brain_slice = val_data_example[i, :, :, im_height//2].detach().cpu().T\n",
        "        plt.xticks([0, im_width - 1], [0, im_width - 1], fontsize=15)\n",
        "        plt.yticks([0, im_length - 1], [0, im_length - 1], fontsize=15)\n",
        "        plt.imshow(brain_slice, cmap=\"gray\")\n",
        "        cbar = plt.colorbar()\n",
        "        cbar.ax.tick_params(labelsize=20)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3e3iR-hKX7T"
      },
      "source": [
        "## Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WScmgodkdmwU"
      },
      "source": [
        "**Define a 3D Unet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeDy37CJdqCT"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "out_channels = 12 if QR_REGRESSION else 8\n",
        "model = UNet(\n",
        "    spatial_dims=3, # 3D\n",
        "    in_channels=4,\n",
        "    out_channels=out_channels,\n",
        "    channels=(4, 8, 16),\n",
        "    strides=(2, 2),\n",
        "    num_res_units=2\n",
        ").to(device)\n",
        "logger.debug(\"Model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and display the total number of parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "# logger.debug(f\"Total number of trainable parameters: {total_params}\")\n",
        "\n",
        "# Print the model architecture\n",
        "# logger.debug(f\"Model Architecture:\\n {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)\n",
        "ep_start = 1\n",
        "\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "if CONTINUE_TRAINING:\n",
        "    load_dir = save_dir\n",
        "    checkpoint = torch.load(os.path.join(load_dir, 'latest_checkpoint.pth'), weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    ep_start = checkpoint['epoch']\n",
        "\n",
        "    with open(os.path.join(load_dir, 'training_info.pkl'), 'rb') as f:\n",
        "        training_info = pickle.load(f)\n",
        "        epoch_loss_values = training_info['epoch_loss_values']\n",
        "        metric_values = training_info['metric_values']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_nll_loss(outputs, target):\n",
        "    # input is 4 channel images, outputs is 8 channel images\n",
        "    outputs_mean = outputs[:, :4, ...]\n",
        "    if not SET_VARIANCE:\n",
        "        log_std = torch.zeros_like(outputs_mean) # sigma = 1\n",
        "    else:\n",
        "        log_std = outputs[:, 4:, ...]\n",
        "        eps = np.log(1e-6)/2 # -6.9\n",
        "\n",
        "        # TODO: should the clamping be with or without autograd?\n",
        "        log_std = log_std.clone()\n",
        "        with torch.no_grad():\n",
        "            log_std.clamp_(min=eps)\n",
        "\n",
        "    cost1 = (target - outputs_mean)**2 / (2*torch.exp(2*log_std))\n",
        "    cost2 = log_std\n",
        "\n",
        "    return torch.mean(cost1 + cost2)\n",
        "\n",
        "def mse_loss(outputs, target):\n",
        "    return torch.nn.functional.mse_loss(outputs[:, :4, ...], target)\n",
        "\n",
        "def qr_loss(out, tgt, q0=0.5, q1=0.841, q2=0.159):\n",
        "    out0 = out[:, :4, ...]\n",
        "    out1 = out[:, 4:8, ...]\n",
        "    out2 = out[:, 8:, ...]\n",
        "    custom_loss0 = torch.sum(torch.max(q0 * (out0 - tgt), (q0 - 1.0) * (out0 - tgt)))\n",
        "    custom_loss1 = torch.sum(torch.max(q1 * (out1 - tgt), (q1 - 1.0) * (out1 - tgt)))\n",
        "    custom_loss2 = torch.sum(torch.max(q2 * (out2 - tgt), (q2 - 1.0) * (out2 - tgt)))\n",
        "\n",
        "    return custom_loss0 + custom_loss1 + custom_loss2\n",
        "\n",
        "def loss_scheduler(epoch):\n",
        "    if QR_REGRESSION:\n",
        "        return qr_loss\n",
        "    else:\n",
        "        if 500 < epoch < 600:\n",
        "            return mse_loss\n",
        "        else:\n",
        "            return gaussian_nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxkPH4GKX7T"
      },
      "outputs": [],
      "source": [
        "mse_metric = MSEMetric(reduction=\"mean\")\n",
        "\n",
        "# define inference method\n",
        "def inference(input):\n",
        "    def _compute(input):\n",
        "        output = model(input)\n",
        "        return output\n",
        "\n",
        "    with torch.amp.autocast('cuda'):\n",
        "        return _compute(input)\n",
        "\n",
        "\n",
        "# use amp to accelerate training\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "# enable cuDNN benchmark\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save params\n",
        "with open(os.path.join(save_dir, 'params.pkl'), 'wb') as f:\n",
        "    pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8xDYlbRKX7T",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "logger.debug(\"Beginning training...\")\n",
        "total_start = time.time()\n",
        "for epoch in range(ep_start, MAX_EPOCHS+1):\n",
        "    epoch_start_time = time.time()\n",
        "    logger.info(\"-\" * 10)\n",
        "    logger.info(f\"epoch {epoch}/{MAX_EPOCHS}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    criterion = loss_scheduler(epoch)\n",
        "    step_start = time.time()\n",
        "    for batch_data in train_loader:\n",
        "        data_loaded_time = time.time() - step_start\n",
        "        step += 1\n",
        "        inputs, mask, id = (\n",
        "            batch_data[\"image\"].to(device),\n",
        "            batch_data[\"mask\"].to(device),\n",
        "            batch_data[\"id\"],\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            target = inputs.clone()\n",
        "            if DO_MASK:\n",
        "                inputs = inputs*~mask[:,:,None,None,None]\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                    print(f\"NaN found in gradient of parameter: {name}\")\n",
        "                    exit()\n",
        "            outputs = model(inputs)            \n",
        "            loss = criterion(outputs, target)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if epoch > 10 and loss.item() > 1e5:\n",
        "            logger.warning(f\"large loss encountered: {loss.item()}!\")\n",
        "            exit()\n",
        "        if np.isnan(loss.item()):\n",
        "            logger.warning(\"nan value encountered!\")\n",
        "            exit()\n",
        "        epoch_loss += loss.item()\n",
        "        logger.info(\n",
        "            f\"{step}/{len(train_loader)}\"\n",
        "            f\", train_loss: {loss.item():.4f}\"\n",
        "            f\", data-load time: {(data_loaded_time):.4f}\"\n",
        "            f\", total-step time: {(time.time() - step_start):.4f}\"\n",
        "        )\n",
        "        step_start = time.time()\n",
        "    lr_scheduler.step()\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    logger.info(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if epoch % VAL_INTERVAL == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_mask = (\n",
        "                    batch_data[\"image\"].to(device),\n",
        "                    batch_data[\"mask\"].to(device),\n",
        "                )\n",
        "                val_target = val_inputs.clone()\n",
        "                val_inputs = val_inputs*~val_mask[:,:,None,None,None]\n",
        "                val_outputs = inference(val_inputs)\n",
        "                val_output_main = val_outputs[:,:4,...]\n",
        "                mse_metric(y_pred=val_output_main, y=val_target)\n",
        "\n",
        "            metric = 1-mse_metric.aggregate().item()\n",
        "            metric_values.append(metric)\n",
        "            mse_metric.reset()\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            }\n",
        "            torch.save(\n",
        "                checkpoint,\n",
        "                os.path.join(save_dir, 'latest_checkpoint.pth'),\n",
        "            )\n",
        "            logger.info(f\"saved latest model at epoch: {epoch}\")\n",
        "\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch\n",
        "                torch.save(\n",
        "                    checkpoint,\n",
        "                    os.path.join(save_dir, 'best_checkpoint.pth'),\n",
        "                )\n",
        "                logger.info(f\"saved new best metric model at epoch: {epoch}\")\n",
        "                \n",
        "            # Save the loss list\n",
        "            with open(os.path.join(save_dir, 'training_info.pkl'), 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'epoch_loss_values': epoch_loss_values,\n",
        "                    'metric_values': metric_values,\n",
        "                }, f)\n",
        "            logger.info(\n",
        "                f\"current epoch: {epoch} current mean mse: {metric:.4f}\"\n",
        "                f\" best mean metric: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "    logger.info(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start_time):.4f}\")\n",
        "total_time = time.time() - total_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnK_38mOKX7T",
        "tags": []
      },
      "outputs": [],
      "source": [
        "logger.info(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "logger.info(f\"Training time: {total_time//MAX_EPOCHS:.1f}s/ep (total: {total_time//3600:.0f}h {(total_time//60)%60:.0f}m)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv-4",
      "language": "python",
      "name": "myenv-4"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
