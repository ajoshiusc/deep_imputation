{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtiAWdGKX7J"
      },
      "source": [
        "# Contrast Synthesis with Uncertainty estimation\n",
        "## Using Quantile Regression and U-Nets\n",
        "\n",
        "The dataset comes from http://medicaldecathlon.com/.  \n",
        "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
        "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
        "Source: BRATS 2016 and 2017 datasets.  \n",
        "Challenge: **Drop some of the modalities randomly and reconstruct it by imputing with a 3D U-Net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.logger import Logger\n",
        "logger = Logger(log_level='DEBUG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set parameters\n",
        "\n",
        "* RUN_ID : set this to prevent overlapped saving of model and data\n",
        "\n",
        "* DO_MASK : Set to True if mask is to be applied while training\n",
        "\n",
        "* MAX_EPOCHS\n",
        "* VAL_INTERVAL : how frequently the validation code should be run\n",
        "* TRAIN_RATIO: proportion of total dataset to be used for training. Rest will be used for validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_ID = 24\n",
        "QR_REGRESSION = True\n",
        "DO_MASK = True\n",
        "MAX_EPOCHS = 6000\n",
        "TRAIN_DATA_SIZE = 200\n",
        "BATCHSIZE_TRAIN = 2\n",
        "VAL_INTERVAL = 10\n",
        "TRAIN_RATIO = 0.8\n",
        "RANDOM_SEED = 0\n",
        "CONTINUE_TRAINING = False\n",
        "ROOT_DIR = \"/scratch1/sachinsa/cont_syn\"\n",
        "\n",
        "# test code sanity (for silly errors)\n",
        "SANITY_CHECK = False\n",
        "if SANITY_CHECK:\n",
        "    RUN_ID = 0\n",
        "    MAX_EPOCHS = 15\n",
        "    TRAIN_DATA_SIZE = 10\n",
        "    VAL_INTERVAL = 2\n",
        "\n",
        "params = {\n",
        "    'RUN_ID': RUN_ID,\n",
        "    'QR_REGRESSION': QR_REGRESSION,\n",
        "    'DO_MASK': DO_MASK,\n",
        "    'MAX_EPOCHS': MAX_EPOCHS,\n",
        "    'ROOT_DIR': ROOT_DIR\n",
        "}\n",
        "\n",
        "logger.info(\"PARAMETERS\\n-----------------\")\n",
        "logger.info(f\"RUN_ID: {RUN_ID}\")\n",
        "logger.info(f\"QR_REGRESSION: {QR_REGRESSION}\")\n",
        "logger.info(f\"DO_MASK: {DO_MASK}\")\n",
        "logger.info(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
        "logger.info(f\"TRAIN_DATA_SIZE: {TRAIN_DATA_SIZE}\")\n",
        "logger.info(f\"BATCHSIZE_TRAIN: {BATCHSIZE_TRAIN}\")\n",
        "logger.info(f\"VAL_INTERVAL: {VAL_INTERVAL}\")\n",
        "logger.info(f\"TRAIN_RATIO: {TRAIN_RATIO}\")\n",
        "logger.info(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
        "logger.info(f\"CONTINUE_TRAINING: {CONTINUE_TRAINING}\")\n",
        "logger.info(f\"ROOT_DIR: {ROOT_DIR}\")\n",
        "print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDR2QhuhKX7O"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uzoUh4uKX7P",
        "outputId": "8bb638f6-dd68-4ffd-84c2-2d30953b9190",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import MSEMetric\n",
        "from monai.utils import set_determinism\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "from utils.dataset import BraTSDataset\n",
        "from utils.model import create_UNet3D, inference\n",
        "from utils.transforms import contr_syn_transform_2  as data_transform\n",
        "\n",
        "# print_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_dir = os.path.join(ROOT_DIR, f\"run_{RUN_ID}\")\n",
        "if not CONTINUE_TRAINING and os.path.exists(save_dir) and os.path.isdir(save_dir) and len(os.listdir(save_dir)) != 0:\n",
        "    logger.warning(f\"{save_dir} already exists. Avoid overwrite by updating RUN_ID.\")\n",
        "    # exit()\n",
        "else:\n",
        "    os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXLaSctjKX7Q"
      },
      "source": [
        "### Set deterministic training for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeNCIs06KX7R"
      },
      "outputs": [],
      "source": [
        "set_determinism(seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dhVJ9qbVbGR"
      },
      "source": [
        "Create training and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = BraTSDataset(\n",
        "    version='2017',\n",
        "    section = 'training',\n",
        "    seed = RANDOM_SEED,\n",
        "    transform = data_transform['train']\n",
        ")\n",
        "\n",
        "val_dataset = BraTSDataset(\n",
        "    version='2017',\n",
        "    section = 'validation',\n",
        "    seed = RANDOM_SEED,\n",
        "    transform = data_transform['val']\n",
        ")\n",
        "\n",
        "# TODO: add logic to get subset inside BraTSDataset\n",
        "if TRAIN_DATA_SIZE:\n",
        "    train_dataset = Subset(train_dataset, list(range(TRAIN_DATA_SIZE)))\n",
        "    val_dataset = Subset(val_dataset, list(range(TRAIN_DATA_SIZE//4)))\n",
        "\n",
        "BATCHSIZE_VAL = BATCHSIZE_TRAIN\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE_TRAIN, shuffle=True,\n",
        "    num_workers=8)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE_VAL, shuffle=False, num_workers=8)\n",
        "\n",
        "logger.debug(\"Data loaded\")\n",
        "logger.debug(f\"Length of dataset: {len(train_dataset)}, {len(val_dataset)}\")\n",
        "logger.debug(f\"Batch-size: {BATCHSIZE_TRAIN}, {BATCHSIZE_VAL}\")\n",
        "logger.debug(f\"Length of data-loaders: {len(train_loader)}, {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load masks\n",
        "mask_root_dir = \"/scratch1/sachinsa/data/masks/brats2017\"\n",
        "train_mask_df = pd.read_csv(os.path.join(mask_root_dir, \"train_mask.csv\"), index_col=0)\n",
        "val_mask_df = pd.read_csv(os.path.join(mask_root_dir, \"val_mask.csv\"), index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3e3iR-hKX7T"
      },
      "source": [
        "## Create Model, Loss, Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WScmgodkdmwU"
      },
      "source": [
        "**Define a 3D Unet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeDy37CJdqCT"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "out_channels = 12 if QR_REGRESSION else 8\n",
        "model = create_UNet3D(out_channels, device)\n",
        "logger.debug(\"Model defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)\n",
        "ep_start = 1\n",
        "\n",
        "epoch_loss_values = []\n",
        "metric_values = []\n",
        "\n",
        "if CONTINUE_TRAINING:\n",
        "    load_dir = save_dir\n",
        "    checkpoint = torch.load(os.path.join(load_dir, 'latest_checkpoint.pth'), weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    ep_start = checkpoint['epoch']\n",
        "\n",
        "    with open(os.path.join(load_dir, 'training_info.pkl'), 'rb') as f:\n",
        "        training_info = pickle.load(f)\n",
        "        epoch_loss_values = training_info['epoch_loss_values']\n",
        "        metric_values = training_info['metric_values']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.loss import qr_loss, mse_loss, gaussian_nll_loss\n",
        "\n",
        "def loss_scheduler(epoch):\n",
        "    if QR_REGRESSION:\n",
        "        return qr_loss\n",
        "    else:\n",
        "        if 500 < epoch < 600:\n",
        "            return mse_loss\n",
        "        else:\n",
        "            return gaussian_nll_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxkPH4GKX7T"
      },
      "outputs": [],
      "source": [
        "mse_metric = MSEMetric(reduction=\"mean\")\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save params\n",
        "with open(os.path.join(save_dir, 'params.pkl'), 'wb') as f:\n",
        "    pickle.dump(params, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8xDYlbRKX7T",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "best_metric = -1\n",
        "best_metric_epoch = -1\n",
        "\n",
        "logger.debug(\"Beginning training...\")\n",
        "total_start = time.time()\n",
        "for epoch in range(ep_start, MAX_EPOCHS+1):\n",
        "    epoch_start_time = time.time()\n",
        "    logger.info(\"-\" * 10)\n",
        "    logger.info(f\"epoch {epoch}/{MAX_EPOCHS}\")\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    step = 0\n",
        "    criterion = loss_scheduler(epoch)\n",
        "    step_start = time.time()\n",
        "    for train_data in train_loader:\n",
        "        data_loaded_time = time.time() - step_start\n",
        "        step += 1\n",
        "        train_inputs, train_ids = (\n",
        "            train_data[\"image\"].to(device),\n",
        "            train_data[\"id\"],\n",
        "        )\n",
        "        train_mask = torch.from_numpy(train_mask_df.loc[train_ids.tolist(), :].values).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast('cuda'):\n",
        "            target = train_inputs.clone()\n",
        "            if DO_MASK:\n",
        "                train_inputs = train_inputs*~train_mask[:,:,None,None,None]\n",
        "            train_outputs = model(train_inputs)            \n",
        "            loss = criterion(train_outputs, target)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        if np.isnan(loss.item()):\n",
        "            logger.warning(\"nan value encountered!\")\n",
        "            exit()\n",
        "        epoch_loss += loss.item()\n",
        "        logger.info(\n",
        "            f\"{step}/{len(train_loader)}\"\n",
        "            f\", train_loss: {loss.item():.4f}\"\n",
        "            f\", data-load time: {(data_loaded_time):.4f}\"\n",
        "            f\", total-step time: {(time.time() - step_start):.4f}\"\n",
        "        )\n",
        "        step_start = time.time()\n",
        "    lr_scheduler.step()\n",
        "    epoch_loss /= step\n",
        "    epoch_loss_values.append(epoch_loss)\n",
        "    logger.info(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    if epoch % VAL_INTERVAL == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "                val_inputs, val_ids = (\n",
        "                    val_data[\"image\"].to(device),\n",
        "                    val_data[\"id\"],\n",
        "                )\n",
        "                val_mask = torch.from_numpy(val_mask_df.loc[val_ids.tolist(), :].values).to(device)\n",
        "                val_target = val_inputs.clone()\n",
        "                if DO_MASK:\n",
        "                    val_inputs = val_inputs*~val_mask[:,:,None,None,None]\n",
        "                val_outputs = inference(val_inputs, model)\n",
        "                val_output_main = val_outputs[:,:4,...]\n",
        "                mse_metric(y_pred=val_output_main, y=val_target)\n",
        "\n",
        "            metric = 1-mse_metric.aggregate().item()\n",
        "            metric_values.append(metric)\n",
        "            mse_metric.reset()\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': lr_scheduler.state_dict(),\n",
        "            }\n",
        "            torch.save(\n",
        "                checkpoint,\n",
        "                os.path.join(save_dir, 'latest_checkpoint.pth'),\n",
        "            )\n",
        "            logger.info(f\"saved latest model at epoch: {epoch}\")\n",
        "\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch\n",
        "                torch.save(\n",
        "                    checkpoint,\n",
        "                    os.path.join(save_dir, 'best_checkpoint.pth'),\n",
        "                )\n",
        "                logger.info(f\"saved new best metric model at epoch: {epoch}\")\n",
        "                \n",
        "            # Save the loss list\n",
        "            with open(os.path.join(save_dir, 'training_info.pkl'), 'wb') as f:\n",
        "                pickle.dump({\n",
        "                    'epoch_loss_values': epoch_loss_values,\n",
        "                    'metric_values': metric_values,\n",
        "                }, f)\n",
        "            logger.info(\n",
        "                f\"current epoch: {epoch} current mean mse: {metric:.4f}\"\n",
        "                f\" best mean metric: {best_metric:.4f}\"\n",
        "                f\" at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "    logger.info(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start_time):.4f}\")\n",
        "total_time = time.time() - total_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnK_38mOKX7T",
        "tags": []
      },
      "outputs": [],
      "source": [
        "logger.info(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
        "logger.info(f\"Training time: {total_time//MAX_EPOCHS:.1f}s/ep (total: {total_time//3600:.0f}h {(total_time//60)%60:.0f}m)\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv-4",
      "language": "python",
      "name": "myenv-4"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
