{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To analyse synthesis and uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = 111 # set this to prevent overlapped saving of model and data\n",
    "DATA_TRANSFORM = \"SCALE_INTENSITY\"\n",
    "RANDOM_SEED = 0\n",
    "ROOT_DIR = \"/scratch1/sachinsa/cont_syn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.logger import Logger\n",
    "from utils.plot import *\n",
    "\n",
    "logger = Logger(log_level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dir = os.path.join(ROOT_DIR, f\"run_{RUN_ID:03d}\")\n",
    "fig_save_dir_root = os.path.join(\"..\", \"figs\")\n",
    "fig_save_dir = os.path.join(fig_save_dir_root, \"runs\", f\"run_{RUN_ID:03d}\")\n",
    "os.makedirs(fig_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(load_dir, 'training_info.pkl'), 'rb') as f:\n",
    "    training_info = pickle.load(f)\n",
    "    epoch_loss_values = training_info['epoch_loss_values']\n",
    "    metric_values = training_info['metric_values']\n",
    "\n",
    "logger.info(\"PARAMETERS\\n-----------------\")\n",
    "logger.info(f\"RUN_ID: {RUN_ID:03d}\")\n",
    "logger.info(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = len(epoch_loss_values)\n",
    "print(\"Max epochs:\", max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_synth(epoch_loss_values, metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils.model import create_UNet3D, inference\n",
    "from utils.dataset import BraTSDataset\n",
    "if DATA_TRANSFORM == \"SCALE_INTENSITY\":\n",
    "    from utils.transforms import contr_syn_transform_scale as data_transform\n",
    "else:\n",
    "    from utils.transforms import contr_syn_transform_default as data_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "out_channels = 1\n",
    "model = create_UNet3D(3, out_channels, device, data_transform=DATA_TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = BraTSDataset(\n",
    "    version='2017',\n",
    "    section = 'all', #'validation',\n",
    "    seed = RANDOM_SEED,\n",
    "    transform = data_transform['val']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_index = 0\n",
    "index_list = val_dataset.get_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = index_list[id_index]\n",
    "id_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(load_dir, 'best_checkpoint.pth'), weights_only=True)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "channels = [\"FLAIR\", \"T1w\", \"T1Gd\", \"T2w\"]\n",
    "label_list = [\"TC\", \"WT\", \"ET\"]\n",
    "input_mask = [False, False, True, False]\n",
    "input_mask = np.array(input_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    this_input = val_dataset.get_with_id(id_)\n",
    "    input_image = this_input[\"image\"].unsqueeze(0).to(device)\n",
    "    input_image_copy = input_image.clone()\n",
    "    input_image = input_image[:, [0,1,3], ...]\n",
    "    this_output = inference(input_image, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.plot import BrainPlot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Wedge\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def plot_donut(value, title, color, ax):\n",
    "    value = max(0, min(1, value))\n",
    "    ax.axis(\"equal\")\n",
    "\n",
    "    radius = 0.7\n",
    "    inner_radius = 0.5\n",
    "    angle = 360 * value\n",
    "\n",
    "    background_circle = Wedge((0, 0), radius, 0, 360, width=radius - inner_radius, color=\"lightgray\")\n",
    "    ax.add_patch(background_circle)\n",
    "\n",
    "    active_segment = Wedge((0, 0), radius, 0, angle, width=radius - inner_radius, color=color)\n",
    "    ax.add_patch(active_segment)\n",
    "\n",
    "    ax.text(0, 0, f\"{value:.0%}\", ha=\"center\", va=\"center\", fontsize=20, color=\"black\")\n",
    "    ax.text(0, 0.85, title, ha=\"center\", va=\"center\", fontsize=20, color=\"black\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "\n",
    "def tensor_normalize(tensor):\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def tensor_clamp(tensor, low=0.01, hi=0.99):\n",
    "    lower_bound = torch.quantile(tensor, low)\n",
    "    upper_bound = torch.quantile(tensor, hi)\n",
    "    return torch.clamp(tensor, min=lower_bound.item(), max=upper_bound.item())\n",
    "\n",
    "def background_standardize(tensor):\n",
    "    max_ = tensor.max().item()\n",
    "    min_ = tensor.min().item()\n",
    "    tensor[torch.abs(tensor) <  0.03] = min_ # (max_+min_)/2\n",
    "    return tensor\n",
    "\n",
    "class BrainPlot():\n",
    "    def __init__(self, input_image, output_image, input_mask, id, h_index=None, clamp_vis=True):\n",
    "        self.input_image = input_image\n",
    "        self.output_image = output_image\n",
    "        self.input_mask = input_mask\n",
    "        self.id = id\n",
    "        im_shape = input_image.shape[2:]\n",
    "        self.im_length = im_shape[0]\n",
    "        self.im_width = im_shape[1]\n",
    "        self.im_height = im_shape[2]\n",
    "        self.h_index = h_index if h_index is not None else self.im_height//2\n",
    "        self.clamp_vis = clamp_vis\n",
    "        self.channels = [\"FLAIR\", \"T1w\", \"T1Gd\", \"T2w\"]\n",
    "        self.num_channels = len(self.channels)\n",
    "        self.total_index = 0\n",
    "\n",
    "    def t1gd_plot(self, key):\n",
    "        row_title = \"\"\n",
    "        this_input_sub = self.input_image[0, :, :, :, self.h_index]\n",
    "        this_output_sub = self.output_image[0, :, :, :, self.h_index]\n",
    "        nc = self.num_channels\n",
    "\n",
    "        if key == \"FLAIR\":\n",
    "            brain_slice = this_input_sub[0]\n",
    "        elif key == \"T1w\":\n",
    "            brain_slice = this_input_sub[1]\n",
    "        elif key == \"T2w\":\n",
    "            brain_slice = this_input_sub[3]\n",
    "        elif key == \"T1Gd\":\n",
    "            brain_slice = this_input_sub[2]\n",
    "        elif key == \"T1Gd-synth\":\n",
    "            brain_slice = this_output_sub[0]\n",
    "\n",
    "        plt.subplot(1, 5, 1+self.total_index)\n",
    "        self.total_index += 1\n",
    "\n",
    "        brain_slice = brain_slice.detach().cpu().T\n",
    "        # brain_slice = torch.flip(brain_slice, dims=[0])\n",
    "        # if brain_slice.dtype != torch.bool:\n",
    "        #     if self.clamp_vis:\n",
    "        #         brain_slice = tensor_clamp(brain_slice)\n",
    "            # brain_slice = background_standardize(brain_slice)\n",
    "\n",
    "        col_title = key\n",
    "        plt.title(col_title, fontsize=30)\n",
    "\n",
    "        plt.imshow(brain_slice, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        plt.xlabel('')\n",
    "        plt.suptitle(f\"BRATS_{self.id} (h={self.h_index}/{self.im_height})\", fontsize=20)\n",
    "        # plt.xticks([self.im_width - 1], [self.im_width], fontsize=15)\n",
    "        # plt.yticks([self.im_length - 1], [self.im_length], fontsize=15)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # plt.colorbar()\n",
    "        # plt.tight_layout()\n",
    "        # cbar = plt.colorbar(shrink=0.7)\n",
    "        # cbar.ax.tick_params(labelsize=20)\n",
    "\n",
    "    def plot(self, key):\n",
    "        index = self.total_index%self.num_channels\n",
    "        row_title = \"\"\n",
    "        this_input_sub = self.input_image[0, :, :, :, self.h_index]\n",
    "        this_output_sub = self.output_image[0, :, :, :, self.h_index]\n",
    "        nc = self.num_channels\n",
    "\n",
    "        if key == \"input\":\n",
    "            row_title = \"Input\"\n",
    "            brain_slice = this_input_sub[index]\n",
    "        elif key == \"q0\":\n",
    "            row_title = \"Output: \\n\" + \"Median\"\n",
    "            brain_slice = this_output_sub[index]\n",
    "        elif key == \"q1\":\n",
    "            row_title = \"Output: \\n\" + r\"qL\" + \"\"\n",
    "            brain_slice = this_output_sub[index+1*nc]\n",
    "        elif key == \"q2\":\n",
    "            row_title = \"Output: \\n\" + r\"qH\" + \"\"\n",
    "            brain_slice = this_output_sub[index+2*nc]\n",
    "        elif key == \"q3\":\n",
    "            row_title = \"Outlier\"\n",
    "            lower_slice = this_input_sub[index] < this_output_sub[index+nc]\n",
    "            upper_slice = this_input_sub[index] > this_output_sub[index+2*nc]\n",
    "            brain_slice = torch.logical_or(lower_slice, upper_slice)\n",
    "        elif key == \"diff\":\n",
    "            row_title = \"Diff\"\n",
    "            brain_slice = this_input_sub[index]-this_output_sub[index]\n",
    "        num_rows = 5\n",
    "        \n",
    "        plt.subplot(num_rows, 4, 1+self.total_index)\n",
    "        self.total_index += 1\n",
    "\n",
    "        col_title = self.channels[index]\n",
    "        col_color = \"red\" if self.input_mask[index] else \"green\"\n",
    "        if key == \"input\":\n",
    "            plt.title(col_title, fontsize=30, color=col_color)\n",
    "        brain_slice = brain_slice.detach().cpu().T\n",
    "        brain_slice = torch.flip(brain_slice, dims=[0]) # flip horizontally\n",
    "\n",
    "        if brain_slice.dtype != torch.bool:\n",
    "            if self.clamp_vis:\n",
    "                brain_slice = tensor_clamp(brain_slice)\n",
    "            brain_slice = background_standardize(brain_slice)\n",
    "\n",
    "        plt.imshow(brain_slice, cmap=\"gray\")\n",
    "\n",
    "        plt.xlabel('')\n",
    "        if index == 0:\n",
    "            plt.ylabel(row_title, fontsize=30)\n",
    "\n",
    "        plt.suptitle(f\"BRATS_{self.id} (h={self.h_index}/{self.im_height})\", fontsize=20)\n",
    "        plt.xticks([self.im_width - 1], [self.im_width], fontsize=15)\n",
    "        plt.yticks([self.im_length - 1], [self.im_length], fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        cbar = plt.colorbar(shrink=0.7)\n",
    "        cbar.ax.tick_params(labelsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"image\", (18, 4))\n",
    "b_plot = BrainPlot(input_image_copy, this_output, input_mask, h_index=86, id=id_)\n",
    "\n",
    "b_plot.t1gd_plot(\"FLAIR\")\n",
    "b_plot.t1gd_plot(\"T1w\")\n",
    "b_plot.t1gd_plot(\"T2w\")\n",
    "b_plot.t1gd_plot(\"T1Gd\")\n",
    "b_plot.t1gd_plot(\"T1Gd-synth\")\n",
    "\n",
    "plt.savefig(os.path.join(fig_save_dir_root, \"results\", f\"brats_{id_}.png\"), facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import DataLoader\n",
    "import torch.nn.functional as torch_F\n",
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "\n",
    "def compute_ssim(predicted, ground):\n",
    "    return structural_similarity_index_measure(predicted, ground, data_range=ground.max() - ground.min()).item()\n",
    "\n",
    "def filter_with_mask(data, mask):\n",
    "    return data[:, mask, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4)\n",
    "print(len(val_dataset.get_ids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_metric(is_masked):\n",
    "#     ssim_list = []\n",
    "#     psnr_list = []\n",
    "\n",
    "#     model.eval()\n",
    "#     i = 0\n",
    "#     with torch.no_grad():\n",
    "#         for this_input in val_loader:\n",
    "#             i+=1\n",
    "#             if i > 10: break\n",
    "#             this_input_image = this_input[\"image\"].to(device)\n",
    "#             this_groundtruth = this_input_image.clone()[:, [2], ...] # T1Gd\n",
    "#             this_input_image = this_input_image[:,input_filter, ...]\n",
    "#             this_predicted = inference(this_input_image, model)\n",
    "\n",
    "#             ssim_list.append(\n",
    "#                 compute_ssim(this_predicted, this_groundtruth)\n",
    "#             )\n",
    "\n",
    "#             mse_loss_val = torch_F.mse_loss(this_predicted, this_groundtruth)\n",
    "#             peak_signal_value = torch.max(this_groundtruth) - torch.min(this_groundtruth)\n",
    "#             psnr_score = 20 * torch.log10(peak_signal_value / torch.sqrt(mse_loss_val))\n",
    "#             psnr_list.append(psnr_score.item())\n",
    "\n",
    "#     return np.mean(psnr_list), np.mean(ssim_list)\n",
    "\n",
    "# psnr_score, ssim_score = evaluate_metric(is_masked=None)\n",
    "# print(\"Overall score:\")\n",
    "# print(f\"PSNR: {psnr_score:.1f}\")\n",
    "# print(f\"SSIM: {ssim_score:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
