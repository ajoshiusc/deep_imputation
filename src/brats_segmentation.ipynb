{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# Brain tumor 3D segmentation with MONAI\n",
    "\n",
    "This tutorial shows how to construct a training workflow of multi-labels segmentation task.\n",
    "\n",
    "And it contains below features:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly adjust intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. 3D SegResNet model, Dice loss function, Mean Dice metric for 3D segmentation task.\n",
    "1. Deterministic training for reproducibility.\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "Below figure shows image patches with the tumor sub-regions that are annotated in the different modalities (top left) and the final labels for the whole dataset (right).\n",
    "(Figure taken from the [BraTS IEEE TMI paper](https://ieeexplore.ieee.org/document/6975210/))\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue).\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/brats_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 3\n",
    "ROOT_DIR = \"/scratch1/sachinsa/brats_seg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "# import onnxruntime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new transform to convert brain tumor labels\n",
    "\n",
    "Here we convert the multi-classes labels into multi-labels segmentation task in One-Hot format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 1 is the peritumoral edema\n",
    "    label 2 is the GD-enhancing tumor\n",
    "    label 3 is the necrotic and non-enhancing tumor core\n",
    "    The possible classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(torch.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(torch.logical_or(torch.logical_or(d[key] == 2, d[key] == 3), d[key] == 1))\n",
    "            # label 2 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose(\n",
    "    [\n",
    "        # load 4 Nifti images and stack them together\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[224, 224, 144], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "    ]\n",
    ")\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly load data with DecathlonDataset\n",
    "\n",
    "Here we use `DecathlonDataset` to automatically download and extract the dataset.\n",
    "It inherits MONAI `CacheDataset`, if you want to use less memory, you can set `cache_num=N` to cache N items for training and use the default args to cache all the items for validation, it depends on your memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we don't cache any data in case out of memory issue\n",
    "train_ds = DecathlonDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=train_transform,\n",
    "    section=\"training\",\n",
    "    download=True,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "val_ds = DecathlonDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    task=\"Task01_BrainTumour\",\n",
    "    transform=val_transform,\n",
    "    section=\"validation\",\n",
    "    download=False,\n",
    "    cache_rate=0.0,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from DecathlonDataset to visualize and check the 4 channels\n",
    "val_data_example = val_ds[2]\n",
    "print(f\"image shape: {val_data_example['image'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.title(f\"image channel {i}\")\n",
    "    plt.imshow(val_data_example[\"image\"][i, :, :, 60].detach().cpu(), cmap=\"gray\")\n",
    "plt.show()\n",
    "# also visualize the 3 channels label corresponding to this image\n",
    "print(f\"label shape: {val_data_example['label'].shape}\")\n",
    "plt.figure(\"label\", (18, 6))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(f\"label channel {i}\")\n",
    "    plt.imshow(val_data_example[\"label\"][i, :, :, 60].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "device = torch.device(\"cuda:0\")\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4],\n",
    "    blocks_up=[1, 1, 1],\n",
    "    init_filters=16,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    dropout_prob=0.2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(240, 240, 160),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute a typical PyTorch training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(ROOT_DIR, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(ROOT_DIR, \"best_metric_model.pth\"), weights_only=True))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    i = 0\n",
    "    for val_data in val_loader:\n",
    "        i += 1\n",
    "        if i > 4: break\n",
    "        val_inputs, val_labels = (\n",
    "            val_data[\"image\"].to(device),\n",
    "            val_data[\"label\"].to(device),\n",
    "        )\n",
    "        val_outputs = inference(val_inputs)\n",
    "        val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    metric = dice_metric.aggregate().item()\n",
    "    metric_batch_org = dice_metric_batch.aggregate()\n",
    "\n",
    "    dice_metric.reset()\n",
    "    dice_metric_batch.reset()\n",
    "\n",
    "metric_tc, metric_wt, metric_et = metric_batch_org[0].item(), metric_batch_org[1].item(), metric_batch_org[2].item()\n",
    "\n",
    "print(f\"metric: {metric:.4f}\")\n",
    "print(f\"metric_tc: {metric_tc:.4f}\")\n",
    "print(f\"metric_wt: {metric_wt:.4f}\")\n",
    "print(f\"metric_et: {metric_et:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check best pytorch model output with the input image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(ROOT_DIR, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # select one image to evaluate and visualize the model output\n",
    "    val_input = val_ds[6][\"image\"].unsqueeze(0).to(device)\n",
    "    roi_size = (128, 128, 64)\n",
    "    sw_batch_size = 4\n",
    "    val_output = inference(val_input)\n",
    "    val_output = post_trans(val_output[0])\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"image channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"image\"][i, :, :, 70].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # visualize the 3 channels label corresponding to this image\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"label channel {i}\")\n",
    "        plt.imshow(val_ds[6][\"label\"][i, :, :, 70].detach().cpu())\n",
    "    plt.show()\n",
    "    # visualize the 3 channels model output corresponding to this image\n",
    "    plt.figure(\"output\", (18, 6))\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"output channel {i}\")\n",
    "        plt.imshow(val_output[i, :, :, 70].detach().cpu())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv-4",
   "language": "python",
   "name": "myenv-4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
