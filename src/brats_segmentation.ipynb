{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain tumor 3D segmentation with MONAI\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Logging level is: DEBUG\n"
     ]
    }
   ],
   "source": [
    "from utils.logger import Logger\n",
    "logger = Logger(log_level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] PARAMETERS\n",
      "-----------------\n",
      "[INFO] RUN_ID: 0\n",
      "[INFO] MASK_CODE: 1\n",
      "[INFO] MAX_EPOCHS: 2\n",
      "[INFO] TRAIN_DATA_SIZE: 6\n",
      "[INFO] BATCHSIZE_TRAIN: 2\n",
      "[INFO] VAL_INTERVAL: 1\n",
      "[INFO] RANDOM_SEED: 0\n",
      "[INFO] ROOT_DIR: /scratch1/sachinsa/brats_seg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RUN_ID = 80\n",
    "# USE_PROCESSED = True\n",
    "# ONLY_MEDIAN = True\n",
    "# DO_MASK = False\n",
    "MASK_CODE = RUN_ID - 80\n",
    "RANDOM_SEED = 0\n",
    "MAX_EPOCHS = 2000\n",
    "TRAIN_DATA_SIZE = None\n",
    "VAL_INTERVAL = 5\n",
    "BATCHSIZE_TRAIN = 2\n",
    "ROOT_DIR = \"/scratch1/sachinsa/brats_seg\"\n",
    "DATA_ROOT_DIR = \"/scratch1/sachinsa/data\"\n",
    "\n",
    "# test code sanity (for silly errors)\n",
    "SANITY_CHECK = False\n",
    "if SANITY_CHECK:\n",
    "    RUN_ID = 0\n",
    "    MAX_EPOCHS = 2\n",
    "    TRAIN_DATA_SIZE = 6\n",
    "    VAL_INTERVAL = 1\n",
    "\n",
    "logger.info(\"PARAMETERS\\n-----------------\")\n",
    "logger.info(f\"RUN_ID: {RUN_ID}\")\n",
    "# logger.info(f\"USE_PROCESSED: {USE_PROCESSED}\")\n",
    "# logger.info(f\"ONLY_MEDIAN: {ONLY_MEDIAN}\")\n",
    "# logger.info(f\"DO_MASK: {DO_MASK}\")\n",
    "logger.info(f\"MASK_CODE: {MASK_CODE}\")\n",
    "logger.info(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
    "logger.info(f\"TRAIN_DATA_SIZE: {TRAIN_DATA_SIZE}\")\n",
    "logger.info(f\"BATCHSIZE_TRAIN: {BATCHSIZE_TRAIN}\")\n",
    "logger.info(f\"VAL_INTERVAL: {VAL_INTERVAL}\")\n",
    "logger.info(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
    "logger.info(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from utils.dataset import BraTSDataset\n",
    "from utils.model import create_SegResNet, inference\n",
    "from utils.transforms import tumor_seg_transform_3 as data_transform\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] /scratch1/sachinsa/brats_seg/run_0 already exists. Avoid overwrite by updating RUN_ID.\n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(ROOT_DIR, f\"run_{RUN_ID}\")\n",
    "if os.path.exists(save_dir) and os.path.isdir(save_dir) and len(os.listdir(save_dir)) != 0:\n",
    "    logger.warning(f\"{save_dir} already exists. Avoid overwrite by updating RUN_ID.\")\n",
    "    # exit()\n",
    "else:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Data loaded\n",
      "[DEBUG] Length of dataset: 6, 1\n",
      "[DEBUG] Batch-size: 2, 2\n",
      "[DEBUG] Length of data-loaders: 3, 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/sachinsa/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 7, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = BraTSDataset(\n",
    "    version='2017',\n",
    "    # processed = USE_PROCESSED,\n",
    "    section = 'training',\n",
    "    seed = RANDOM_SEED,\n",
    "    transform = data_transform['train']\n",
    ")\n",
    "\n",
    "val_dataset = BraTSDataset(\n",
    "    version='2017',\n",
    "    # processed = USE_PROCESSED,\n",
    "    section = 'validation',\n",
    "    seed = RANDOM_SEED,\n",
    "    transform = data_transform['val']\n",
    ")\n",
    "\n",
    "# TODO: add logic to get subset inside BraTSDataset\n",
    "if TRAIN_DATA_SIZE:\n",
    "    train_dataset = Subset(train_dataset, list(range(TRAIN_DATA_SIZE)))\n",
    "    val_dataset = Subset(val_dataset, list(range(TRAIN_DATA_SIZE//4)))\n",
    "\n",
    "BATCHSIZE_VAL = BATCHSIZE_TRAIN\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE_TRAIN, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE_VAL, shuffle=False, num_workers=8)\n",
    "\n",
    "logger.debug(\"Data loaded\")\n",
    "logger.debug(f\"Length of dataset: {len(train_dataset)}, {len(val_dataset)}\")\n",
    "logger.debug(f\"Batch-size: {BATCHSIZE_TRAIN}, {BATCHSIZE_VAL}\")\n",
    "logger.debug(f\"Length of data-loaders: {len(train_loader)}, {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load masks\n",
    "# mask_root_dir = \"/scratch1/sachinsa/data/masks/brats2017\"\n",
    "# train_mask_df = pd.read_csv(os.path.join(mask_root_dir, \"train_mask.csv\"), index_col=0)\n",
    "# val_mask_df = pd.read_csv(os.path.join(mask_root_dir, \"val_mask.csv\"), index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Masked contrasts: ['FLAIR']\n"
     ]
    }
   ],
   "source": [
    "def all_subsets(arr):\n",
    "    subsets = list(chain.from_iterable(combinations(arr, r) for r in range(0, len(arr))))\n",
    "    return [list(subset) for subset in subsets]\n",
    "\n",
    "mask_indices = all_subsets([0, 1, 2, 3])[MASK_CODE]\n",
    "show_indices = [x for x in [0, 1, 2, 3] if x not in mask_indices]\n",
    "channels = [\"FLAIR\", \"T1w\", \"T1Gd\", \"T2w\"]\n",
    "label_list = [\"TC\", \"WT\", \"ET\"]\n",
    "\n",
    "logger.info(f\"Masked contrasts: {[channels[i] for i in mask_indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a SegResNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Model defined\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "in_channels = len(show_indices)\n",
    "model = create_SegResNet(in_channels, device)\n",
    "logger.debug(\"Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Beginning training...\n",
      "----------\n",
      "epoch 1/2\n",
      "[INFO] 1/3, train_loss: 0.9572, data-load time: 3.3135, total-step time: 24.5934\n",
      "[INFO] 2/3, train_loss: 0.9604, data-load time: 0.0396, total-step time: 1.5087\n",
      "[INFO] 3/3, train_loss: 0.9779, data-load time: 0.0139, total-step time: 1.4889\n",
      "epoch 1 average loss: 0.9652\n",
      "[INFO] saved new best metric model at epoch: 1\n",
      "current epoch: 1 current mean dice: 0.0160 tc: 0.0007 wt: 0.0474 et: 0.0000\n",
      "best mean dice: 0.0160 at epoch: 1\n",
      "time consuming of epoch 1 is: 37.1168\n",
      "----------\n",
      "epoch 2/2\n",
      "[INFO] 1/3, train_loss: 0.9435, data-load time: 2.8852, total-step time: 4.3675\n",
      "[INFO] 2/3, train_loss: 0.9483, data-load time: 0.0138, total-step time: 1.4858\n",
      "[INFO] 3/3, train_loss: 0.9649, data-load time: 0.0369, total-step time: 1.5133\n",
      "epoch 2 average loss: 0.9522\n",
      "[INFO] saved new best metric model at epoch: 2\n",
      "current epoch: 2 current mean dice: 0.0212 tc: 0.0007 wt: 0.0628 et: 0.0000\n",
      "best mean dice: 0.0212 at epoch: 2\n",
      "time consuming of epoch 2 is: 9.9291\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "logger.debug(\"Beginning training...\")\n",
    "total_start = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    step_start = time.time()\n",
    "    for train_data in train_loader:\n",
    "        data_loaded_time = time.time() - step_start\n",
    "        step += 1\n",
    "        train_inputs, train_labels, train_ids= (\n",
    "            train_data[\"image\"].to(device),\n",
    "            train_data[\"label\"].to(device),\n",
    "            train_data[\"id\"],\n",
    "        )\n",
    "        # train_mask = torch.from_numpy(train_mask_df.loc[train_ids.tolist(), :].values).to(device)\n",
    "        # if USE_PROCESSED and ONLY_MEDIAN:\n",
    "        #     train_inputs = train_inputs[:, :4, ...]\n",
    "        # if DO_MASK:\n",
    "        train_inputs = train_inputs[:, show_indices, ...]\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            train_outputs = model(train_inputs)\n",
    "            loss = loss_function(train_outputs, train_labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        logger.info(\n",
    "            f\"{step}/{len(train_loader)}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", data-load time: {(data_loaded_time):.4f}\"\n",
    "            f\", total-step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "        step_start = time.time()\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % VAL_INTERVAL == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels, val_ids = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                    val_data[\"id\"],\n",
    "                )\n",
    "                # val_mask = torch.from_numpy(val_mask_df.loc[val_ids.tolist(), :].values).to(device)\n",
    "                # if USE_PROCESSED and ONLY_MEDIAN:\n",
    "                #     val_inputs = val_inputs[:, :4, ...]\n",
    "                # if DO_MASK:\n",
    "                val_inputs = val_inputs[:, show_indices, ...]\n",
    "                val_outputs = inference(val_inputs, model)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                }\n",
    "                torch.save(\n",
    "                    checkpoint,\n",
    "                    os.path.join(save_dir, 'best_checkpoint.pth'),\n",
    "                )\n",
    "                logger.info(f\"saved new best metric model at epoch: {epoch}\")\n",
    "            with open(os.path.join(save_dir, 'training_info.pkl'), 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'epoch_loss_values': epoch_loss_values,\n",
    "                    'metric_values': metric_values,\n",
    "                    'metric_values_tc': metric_values_tc,\n",
    "                    'metric_values_wt': metric_values_wt,\n",
    "                    'metric_values_et': metric_values_et\n",
    "                }, f)\n",
    "            print(\n",
    "                f\"current epoch: {epoch} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train completed, best_metric: 0.0212 at epoch: 2\n",
      "[INFO] Training time: 23.0s/ep (total: 0h 0m)\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "logger.info(f\"Training time: {total_time//MAX_EPOCHS:.1f}s/ep (total: {total_time//3600:.0f}h {(total_time//60)%60:.0f}m)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
