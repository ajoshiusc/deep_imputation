{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain tumor 3D segmentation with MONAI\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.  \n",
    "Target: Gliomas segmentation necrotic/active tumour and oedema  \n",
    "Modality: Multimodal multisite MRI data (FLAIR, T1w, T1gd,T2w)  \n",
    "Size: 750 4D volumes (484 Training + 266 Testing)  \n",
    "Source: BRATS 2016 and 2017 datasets.  \n",
    "Challenge: Complex and heterogeneously-located targets\n",
    "\n",
    "The image patches show from left to right:\n",
    "1. the whole tumor (yellow) visible in T2-FLAIR (Fig.A).\n",
    "1. the tumor core (red) visible in T2 (Fig.B).\n",
    "1. the enhancing tumor structures (light blue) visible in T1Gd, surrounding the cystic/necrotic components of the core (green) (Fig. C).\n",
    "1. The segmentations are combined to generate the final labels of the tumor sub-regions (Fig.D): edema (yellow), non-enhancing solid core (red), necrotic/cystic core (green), enhancing core (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.logger import Logger\n",
    "logger = Logger(log_level='DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = 20\n",
    "MASK_CODE = RUN_ID - 20\n",
    "RANDOM_SEED = 0\n",
    "MAX_EPOCHS = 2000\n",
    "TRAIN_DATA_SIZE = None\n",
    "VAL_INTERVAL = 5\n",
    "BATCHSIZE_TRAIN = 2\n",
    "ROOT_DIR = \"/scratch1/sachinsa/brats_seg\"\n",
    "DATA_ROOT_DIR = \"/scratch1/sachinsa/data\"\n",
    "\n",
    "# test code sanity (for silly errors)\n",
    "SANITY_CHECK = False\n",
    "if SANITY_CHECK:\n",
    "    RUN_ID = 0\n",
    "    MAX_EPOCHS = 2\n",
    "    TRAIN_DATA_SIZE = 6\n",
    "    VAL_INTERVAL = 1\n",
    "\n",
    "logger.info(\"PARAMETERS\\n-----------------\")\n",
    "logger.info(f\"RUN_ID: {RUN_ID}\")\n",
    "logger.info(f\"MASK_CODE: {MASK_CODE}\")\n",
    "logger.info(f\"MAX_EPOCHS: {MAX_EPOCHS}\")\n",
    "logger.info(f\"TRAIN_DATA_SIZE: {TRAIN_DATA_SIZE}\")\n",
    "logger.info(f\"BATCHSIZE_TRAIN: {BATCHSIZE_TRAIN}\")\n",
    "logger.info(f\"VAL_INTERVAL: {VAL_INTERVAL}\")\n",
    "logger.info(f\"RANDOM_SEED: {RANDOM_SEED}\")\n",
    "logger.info(f\"ROOT_DIR: {ROOT_DIR}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from utils.dataset import BraTSDataset\n",
    "from utils.model import create_SegResNet, inference\n",
    "from utils.transforms import tumor_seg_transform_3 as data_transform\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(ROOT_DIR, f\"run_{RUN_ID}\")\n",
    "if os.path.exists(save_dir) and os.path.isdir(save_dir) and len(os.listdir(save_dir)) != 0:\n",
    "    logger.warning(f\"{save_dir} already exists. Avoid overwrite by updating RUN_ID.\")\n",
    "    # exit()\n",
    "else:\n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set deterministic training for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_determinism(seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = BraTSDataset(\n",
    "    version='2017',\n",
    "    section = 'training',\n",
    "    load_t1gd = False,\n",
    "    seed = RANDOM_SEED,\n",
    "    transform = data_transform['train']\n",
    ")\n",
    "\n",
    "val_dataset = BraTSDataset(\n",
    "    version='2017',\n",
    "    section = 'validation',\n",
    "    load_t1gd = False,\n",
    "    seed = RANDOM_SEED,\n",
    "    transform = data_transform['val']\n",
    ")\n",
    "\n",
    "# TODO: add logic to get subset inside BraTSDataset\n",
    "if TRAIN_DATA_SIZE:\n",
    "    train_dataset = Subset(train_dataset, list(range(TRAIN_DATA_SIZE)))\n",
    "    val_dataset = Subset(val_dataset, list(range(TRAIN_DATA_SIZE//4)))\n",
    "\n",
    "BATCHSIZE_VAL = BATCHSIZE_TRAIN\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCHSIZE_TRAIN, shuffle=False, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCHSIZE_VAL, shuffle=False, num_workers=8)\n",
    "\n",
    "logger.debug(\"Data loaded\")\n",
    "logger.debug(f\"Length of dataset: {len(train_dataset)}, {len(val_dataset)}\")\n",
    "logger.debug(f\"Batch-size: {BATCHSIZE_TRAIN}, {BATCHSIZE_VAL}\")\n",
    "logger.debug(f\"Length of data-loaders: {len(train_loader)}, {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_subsets(arr):\n",
    "    subsets = list(chain.from_iterable(combinations(arr, r) for r in range(0, len(arr))))\n",
    "    return [list(subset) for subset in subsets]\n",
    "\n",
    "mask_indices = all_subsets([0, 1, 2, 3])[MASK_CODE]\n",
    "show_indices = [x for x in [0, 1, 2, 3] if x not in mask_indices]\n",
    "channels = [\"FLAIR\", \"T1w\", \"T1Gd\", \"T2w\"]\n",
    "label_list = [\"TC\", \"WT\", \"ET\"]\n",
    "\n",
    "logger.info(f\"Masked contrasts: {[channels[i] for i in mask_indices]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model, Loss, Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a SegResNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "in_channels = len(show_indices)\n",
    "model = create_SegResNet(in_channels, device)\n",
    "logger.debug(\"Model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "logger.debug(\"Beginning training...\")\n",
    "total_start = time.time()\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch}/{MAX_EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    step_start = time.time()\n",
    "    for train_data in train_loader:\n",
    "        data_loaded_time = time.time() - step_start\n",
    "        step += 1\n",
    "        train_inputs, train_labels, train_ids= (\n",
    "            train_data[\"image\"].to(device),\n",
    "            train_data[\"label\"].to(device),\n",
    "            train_data[\"id\"],\n",
    "        )\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            train_outputs = model(train_inputs)\n",
    "            loss = loss_function(train_outputs, train_labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        logger.info(\n",
    "            f\"{step}/{len(train_loader)}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", data-load time: {(data_loaded_time):.4f}\"\n",
    "            f\", total-step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "        step_start = time.time()\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if epoch % VAL_INTERVAL == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels, val_ids = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                    val_data[\"id\"],\n",
    "                )\n",
    "\n",
    "                val_outputs = inference(val_inputs, model)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                checkpoint = {\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "                }\n",
    "                torch.save(\n",
    "                    checkpoint,\n",
    "                    os.path.join(save_dir, 'best_checkpoint.pth'),\n",
    "                )\n",
    "                logger.info(f\"saved new best metric model at epoch: {epoch}\")\n",
    "            with open(os.path.join(save_dir, 'training_info.pkl'), 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'epoch_loss_values': epoch_loss_values,\n",
    "                    'metric_values': metric_values,\n",
    "                    'metric_values_tc': metric_values_tc,\n",
    "                    'metric_values_wt': metric_values_wt,\n",
    "                    'metric_values_et': metric_values_et\n",
    "                }, f)\n",
    "            print(\n",
    "                f\"current epoch: {epoch} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "logger.info(f\"Training time: {total_time//MAX_EPOCHS:.1f}s/ep (total: {total_time//3600:.0f}h {(total_time//60)%60:.0f}m)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
